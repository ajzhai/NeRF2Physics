<!DOCTYPE html>
<html>

<head lang="en">
  <!-- <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> -->

  <!-- <meta http-equiv="x-ua-compatible" content="ie=edge"> -->

  <title>NeRF2Physics: Physical Property Understanding from Language-Embedded Feature Fields</title>

  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- mirror: F0%9F%AA%9E&lt -->

  <link rel="stylesheet" type="text/css" href="./files/slick.css">
  <link rel="stylesheet" type="text/css" href="./files/slick-theme.css">
  <link rel="stylesheet" href="./files/bulma.min.css">
  <link rel="stylesheet" href="./files/bulma-slider.min.css">
  <link rel="stylesheet" href="./files/bulma-carousel.min.css">
  <link rel="stylesheet" href="./files/bootstrap.min.css">
  <link rel="stylesheet" href="./files/font-awesome.min.css">
  <link rel="stylesheet" href="./files/codemirror.min.css">
  <link rel="stylesheet" href="./files/app.css">
  <link rel="stylesheet" href="./files/index.css">
  <link rel="stylesheet" href="./files/select.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./files/bootstrap.min.js"></script>
  <script src="./files/codemirror.min.js"></script>
  <script src="./files/clipboard.min.js"></script>
  <script src="./files/video_comparison.js"></script>
  <script src="./files/select.js"></script>
  <script src="./files/bulma-slider.min.js"></script>
  <script src="./files/bulma-carousel.min.js"></script>
  <!-- <script src="./files/app.js"></script> -->
  <script src="./files/index.js"></script>
  <!-- <script src="./files/slick.js"></script> -->

</head>

<body>
  <div class="container" id="header" style="text-align: center; margin: auto;">
    <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
      <h2 class="col-md-12 text-center" id="title">
        <b>NeRF2Physics</b>: Physical Property Understanding from<br>Language-Embedded Feature Fields

      </h2>
      <h2 class="col-md-12 text-center" id="venue" style="font-size:1.75em; margin-top: 2px">
        CVPR 2024
      </h2>
    </div>
  </div>
  <script>
  </script>
  <div class="container" id="main">
    <div class="row">
      <div class="col-sm-10 col-sm-offset-1 text-center">
        <ul class="list-inline">
          <li> Albert J. Zhai </li>
          <li> Yuan Shen </li>
          <li> Emily Y. Chen </li>
          <li> Gloria X. Wang </li>
          <li> Xinlei Wang </li>
          <li> Sheng Wang </li>
          <li> Kaiyu Guan </li>
          <li> Shenlong Wang </li>
        </ul>
        <ul class="list-inline">
          <li> University of Illinois at Urbana-Champaign </li>
        </ul>
      </div>
    </div>

    <div class="row">
      <div class="col-sm-8 col-sm-offset-2 text-center">
        <span class="link-block">
          <a href="https://arxiv.org/abs/2404.04242" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas"
                data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="-24 -32 432 576"
                data-fa-i2svg="">
                <path fill="currentColor"
                  d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
                </path>
              </svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
            </span>
            <span>Paper</span>
          </a>
        </span> &nbsp;
        <span class="link-block">
          <a href="https://github.com/ajzhai/NeRF2Physics" class="external-link button is-normal is-rounded is-dark">
            <span class="icon" >
              <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab"
                data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
                <path fill="currentColor"
                  d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
                </path>
              </svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
            </span>
            <span>Code</span>
          </a>
        </span>
      </div>
    </div>


    <div class="row">
      <div class="col-sm-10 col-sm-offset-1 text-center">
        <br>
        <video id="firstvid" width="100%" controls playsinline="" autoplay="" loop="" muted=""
        poster="files/loading.gif">
          <source src="files/vids/n2p_360_viz.mp4" type="video/mp4">
        </video>
        <br>
      </div>
      
    </div>


    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h3>
          Abstract
        </h3>
        <div class="text-justify">
          Can computers perceive the physical properties of objects solely through vision? Research in cognitive science and vision science has shown that humans excel at identifying materials and estimating their physical properties based purely on visual appearance. In this paper, we present a novel approach for dense prediction of the physical properties of objects using a collection of images. Inspired by how humans reason about physics through vision, we leverage large language models to propose candidate materials for each object. We then construct a language-embedded point cloud and estimate the physical properties of each 3D point using a zero-shot kernel regression approach. Our method is accurate, annotation-free, and applicable to any object in the open world. Experiments demonstrate the effectiveness of the proposed approach in various physical property reasoning tasks, such as estimating the mass of common objects, as well as other properties like friction and hardness.
        </div>
        <br>
        <center>
          <img src="./files/images/full_overview.PNG" class="img-responsive" alt="overview" width="100%"
            style="max-height: 450px;margin:auto;">
        </center>
        <br>
      </div>
    </div>

    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h3>
          Predicting Physical Properties
        </h3>
        <div class="text-justify">
          We visualize input images of objects from the ABO dataset along with our model's CLIP feature PCA components, zero-shot material segmentation, and predicted mass density. 
          Our model makes reasonable predictions of materials across different parts of objects in 3D, allowing for grounded predictions of physical properties. 
        </div>
        <br>
        <center>
          <img src="./files/images/preds_top.png" class="img-responsive" alt="overview" width="100%"
            style="margin:auto;margin-bottom: 0.3cm">
          <img src="./files/images/preds_bottom.png" class="img-responsive" alt="overview" width="100%"
            style="margin:auto;">
        </center>

        <br> <br>
        <div class="text-justify">
          Our method can be used to predict physical properties in an open-vocabulary manner. 
          We show that it can be used to predict mass density, Young's modulus, thermal conductivity, hardness, and friction coefficients, all without supervision.
        </div>
        
        <br>
        <center>
          <img src="./files/images/ym_tc.png" class="img-responsive" alt="overview" width="100%"
            style="margin:auto;">
        </center>
        <br>
      </div>
    </div>

    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h3>
          Physically Realistic Digital Twins
        </h3>
        <div class="text-justify">
          We show that realistic physical interactions can be simulated using mass-aware digital twins created by NeRF2Physics, enabling applications in immersive computing and simulation.
          Here, the ball hits each object with the same initial momentum.
        </div>
        <br>

          <video id="vid2" width="100%" controls playsinline=""  muted="" autoplay="" loop=""
          >
          <source src="files/vids/n2p_col_viz.mp4" type="video/mp4">
        </video>
        <br> <br>
      </div>
    </div>
<!-- 
    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h3>
          References
        </h3>
        <ol>
          <li>
            Victor Schmidt, Alexandra Sasha Luccioni, M ́elisande Teng, Tianyu Zhang, Alexia Reynaud,
            Sunand Raghupathi, Gautier Cosne,
            Adrien Juraver, Vahe Vardanyan, Alex Hernandez-Garcia, Yoshua Bengio.
            Climategan: Raising climate change awareness by generating images of floods. ICLR, 2022.
            <a href="https://github.com/cc-ai/climategan">[code]</a>
          </li>
          <li>
            Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer.
            High-resolution image synthesis with latent diffusion models. In CVPR, 2022.
            <a href="https://github.com/CompVis/stable-diffusion">[code]</a>
          </li>
          <li>
            Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, and Richard
            Zhang.
            Swapping autoencoder for deep image manipulation. NeurIPS, 2020.
            <a href="https://github.com/taesungp/swapping-autoencoder-pytorch">[code]</a>
          </li>
        </ol>
      </div>
    </div> -->
    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h3>
          Acknowledgements
        </h3>

        The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and 
        <a  href="https://climatenerf.github.io">ClimateNeRF</a>.
        <br>

      </div>
    </div>
  </div>
</body>

</html>